# 实现的注意事项
1. 每个Raft实例的状态（log，状态机等）会被并发的Goroutine的事件响应所影响，所以这些状态可以用**共享数据和锁**来并发更新
2. 对于两个时间驱动的时间，分别都需要一个goroutine来响应
3. 选举超时的实现：在Raft结构中维护一个变量，包含着上次听到领导者消息的时间，然后使用**time.Sleep**来定期检查
4. 每一个Raft应该有一个**commitIndex**变量，它来表示是否有已提交的日志，如果有，则会使用**Applych**这个通道发送数据到状态机，更新状态机。这个过程通常使用单个Goroutine和sync.Cond条件变量来实现，以确保适当时候唤醒goroutine和数据的顺序性。
5. 日志匹配特性:即如果两个日志条目具有相同的索引和任期号，那么它们之前的所有日志条目也必须相同。通过在AppendEntries中包含prevLogIndex和prevLogTerm，领导者可以确保跟随者的日志与自己的日志在prevLogIndex之前的部分是一致的。
6. 每次RPC数据发送和接收处理，都应该有自己的Goroutine
7. 领导者在处理回复时必须小心；它必须检查自发送 RPC 以来任期是否已更改，并且必须考虑到来自并发 RPC 的回复可能已更改领导者状态的可能性
8. 更改commit的时候，如果log的term不等于现在节点的term，则无法提交这个新的log。 只能提交最新的log的时候**顺便**提交前面的log
-----
# 关于使用锁的建议
1. 每当你有多个 goroutine 使用的数据，并且至少有一个 goroutine 可能会修改这些数据时，goroutine 应该使用锁来防止数据的同时使用。
2. 我们需要在整个更新序列中持续持有锁。所有其他使用 rf.currentTerm 或 rf.state 的代码也必须持有锁，以确保所有使用的独占访问。
3. 此外，currentTerm 的每一次使用都必须持有锁，以确保没有其他 goroutine 在我们的临界区中修改 currentTerm。
4. 通常在执行任何可能等待的操作时持有锁是一个坏主意：读取 Go 通道、发送通道、等待计时器、调用 time.Sleep() 或发送 RPC（并等待回复）。**避免死锁：占有等待** 等待的代码应该首先释放锁。如果这不太方便，有时创建一个单独的 goroutine 来执行等待是有用的。

---

# 实现时踩的坑
## Test 3A
* A部分测试很简单，跟着论文图二走就可以。是**必须跟着**图二走，每一条都不能落下。
* 需要注意的是 要考虑有两个或多个节点同时开始选举的情况。

---
## Test 3B
Raft的核心内容：日志复制。

* 更改commit的时候，如果log的term不等于现在节点的term，则无法提交这个新的log。 只能提交最新的log的时候**顺便**提交前面的log。 这是我踩的最大的坑。这是论文图8中专门提到的一个现象，一定要多多注意。
* 检测完Follower日志和Leader日志有冲突后，我们要删除冲突的日志及其以后的条目。随后添加发送来的新entries。 添加过程中检查entries的第一个索引和FOllower的最后一个索引是否一致。
* 日志一致性：因为每次同步日志都要检查PreLogindex和PreLogTer双方相不相等，所以达成日志一致性，只要PreLogindex和PreLogTer双方相等，则保证了从日志开始到Prelogindex的日志是相等的。这是类似递归的思想，因为从第一个日志开始就是这样规定日志同步：PreLogindex和PreLogTer双方相等。
*  Section 5.4 Safety Property：一旦一个日志条目在某个term内被提交，未来的所有Leader必须保留这个条目，并且在相同位置不会改变。 这是因为发送Appendentries时，当大多数Follower有这个条目才会被提交（数据的持久性,且可以保障大多数Follower都提交它）。同时只有拥有最新日志的节点才有资格成为Leader（这就保证了已提交条目不会被新的Leader丢弃或覆盖）。
---
## Test 3C
- 这个比较简单，只需要在每一次改变持久化状态后使用一次persist（）就行。
- 它的一个测试:figure8 /figure8（unreliable）很容易卡住。此时不是因为代码错误，是因为**代码性能不够**。你需要回去看看助教所说的日志复制的优化办法。

---
## Test 3D
因为我个人实现的原因，3D我调试了很久。
- 在3C基础上把persist()函数改一改。rf.persister.Save(raftstate, rf.persister.ReadSnapshot())，避免**空快照**
- 整个日志压缩的过程都离不开lastincludedindex这个变量，它代表中快照中最新日志的索引。3D的测试中有一个crash，crash恢复后是要重新运行Make函数创造Raft节点的。 此时需要注意重新Make后需要把lastApplied赋值成lastincludedindex。不然crash测试会报错的。
- 3D工作量很大，都是索引的处理。 每次处理索引都要问自己，这么处理会不会超出索引范围？ 要怎么避免超出索引呢？

# 思考
## 为什么助教说的日志回溯可以优化？
> 若 follower 没有 prevLogIndex 处的日志，则直接置 conflictIndex = len(log)，conflictTerm = None；

leader 收到返回体后，肯定找不到对应的 term，则设置nextIndex = conflictIndex；
其实就是 leader 对应的 nextIndex 直接回退到该 follower 的日志条目末尾处，因为 prevLogIndex 超前了
若 follower 有 prevLogIndex 处的日志，但是 term 不匹配；则设置 conlictTerm为 prevLogIndex 处的 term，且肯定可以找到日志中该 term出现的第一个日志条目的下标，并置conflictIndex = firstIndexWithTerm；

leader 收到返回体后，有可能找不到对应的 term，即 leader 和 follower 在conflictIndex处以及之后的日志都有冲突，都不能要了，直接置nextIndex = conflictIndex
若找到了对应的term，则找到对应term出现的最后一个日志条目的下一个日志条目，即置nextIndex = lastIndexWithTerm+1；这里其实是默认了若 leader 和 follower 同时拥有该 term 的日志，则不会有冲突，直接取下一个 term 作为日志发起就好，是源自于 5.4 safety 的安全性保证
如果还有冲突，leader 和 follower 会一直根据以上规则回溯 nextIndex

**我不理解的就是：若找到了对应的term，则找到对应term出现的最后一个日志条目的下一个日志条目，即置nextIndex = lastIndexWithTerm+1**
后面我思考的结果结果是： 因为Leader的选举限制致使Leader的日志一定是最新最全的，又因为日志是按照Term顺序存储。 所以当Leader和Follower日志冲突的时候，Leader的冲突日志条目一定会比Follower大。 此时Leader找到和Follower conflictTerm一样term的条目就实现了**快速定位**，比原先一个个减少nextindex要快多了。 
但是定位的结果不一定就会匹配，如若不匹配leader 和 follower 会一直根据以上规则回溯 nextIndex